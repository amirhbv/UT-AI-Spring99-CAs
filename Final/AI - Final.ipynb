{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI - Final\n",
    "## Regression\n",
    "\n",
    "### AmirHossein Habibvand - 810196447\n",
    "\n",
    "In this assignment we are going to predict product prices from ads. We have a dataset from `divar`, advertisement\n",
    "platform, which has title, description, brand, and price for products. Here all of our products are cell phones.\n",
    "\n",
    "There 7 main steps in machine learning problems:\n",
    "- Gathering data\n",
    "- Preparing that data\n",
    "- Choosing a model\n",
    "- Training\n",
    "- Evaluation\n",
    "- Hyperparameter tuning\n",
    "- Prediction\n",
    "\n",
    "We will go through them.\n",
    "\n",
    "### Gathering data\n",
    "We have the dataset from divar, which can be downloaded from [here](https://research.cafebazaar.ir/visage/divar_datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import hazm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "from nltk.corpus import wordnet\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (LabelEncoder, MinMaxScaler, OneHotEncoder,\n",
    "                                   StandardScaler)\n",
    "from sklearn.svm import SVR\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing that data\n",
    "\n",
    "The data is in csv format, lets load the data in pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>brand</th>\n",
       "      <th>city</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>image_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nokia::نوکیا</td>\n",
       "      <td>Qom</td>\n",
       "      <td>نوکیا6303</td>\n",
       "      <td>سلام.یه گوشیه6303سالم که فقط دوتا خط کوچیک رو ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday 07AM</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple::اپل</td>\n",
       "      <td>Tehran</td>\n",
       "      <td>ایفون ٥اس٣٢گیگ</td>\n",
       "      <td>درحد نو سالم اصلى بدون ضربه مهلت تست میدم</td>\n",
       "      <td>0</td>\n",
       "      <td>Wednesday 11AM</td>\n",
       "      <td>1150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Samsung::سامسونگ</td>\n",
       "      <td>Mashhad</td>\n",
       "      <td>سامسونگ j5</td>\n",
       "      <td>گوشى بسیار بسیار تمیز و فقط سه هفته کارکرده و ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday 02PM</td>\n",
       "      <td>590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Apple::اپل</td>\n",
       "      <td>Karaj</td>\n",
       "      <td>گرى 5s ایفون  32گیگ</td>\n",
       "      <td>گلس پشت و رو .کارت اپل ای دی. لوازم جانبی اصلی...</td>\n",
       "      <td>3</td>\n",
       "      <td>Wednesday 04PM</td>\n",
       "      <td>1100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Samsung::سامسونگ</td>\n",
       "      <td>Tehran</td>\n",
       "      <td>galaxy S5 Gold در حد آک</td>\n",
       "      <td>کاملا تمیز و بدون حتی 1 خط و خش\\nبه همراه گلاس...</td>\n",
       "      <td>2</td>\n",
       "      <td>Friday 01PM</td>\n",
       "      <td>900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             brand     city                    title  \\\n",
       "0           0      Nokia::نوکیا      Qom                نوکیا6303   \n",
       "1           1        Apple::اپل   Tehran           ایفون ٥اس٣٢گیگ   \n",
       "2           2  Samsung::سامسونگ  Mashhad               سامسونگ j5   \n",
       "3           3        Apple::اپل    Karaj      گرى 5s ایفون  32گیگ   \n",
       "4           4  Samsung::سامسونگ   Tehran  galaxy S5 Gold در حد آک   \n",
       "\n",
       "                                                desc  image_count  \\\n",
       "0  سلام.یه گوشیه6303سالم که فقط دوتا خط کوچیک رو ...            2   \n",
       "1          درحد نو سالم اصلى بدون ضربه مهلت تست میدم            0   \n",
       "2  گوشى بسیار بسیار تمیز و فقط سه هفته کارکرده و ...            2   \n",
       "3  گلس پشت و رو .کارت اپل ای دی. لوازم جانبی اصلی...            3   \n",
       "4  کاملا تمیز و بدون حتی 1 خط و خش\\nبه همراه گلاس...            2   \n",
       "\n",
       "       created_at    price  \n",
       "0  Wednesday 07AM    60000  \n",
       "1  Wednesday 11AM  1150000  \n",
       "2  Wednesday 02PM   590000  \n",
       "3  Wednesday 04PM  1100000  \n",
       "4     Friday 01PM   900000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./mobile_phone_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has 8 columns, let's define some constant to use afterward, out target is price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_TARGET = 'price'\n",
    "COLUMN_BRAND = 'brand'\n",
    "COLUMN_CITY = 'city'\n",
    "COLUMN_TITLE = 'title'\n",
    "COLUMN_DESCRIPTION = 'desc'\n",
    "COLUMN_IMAGE_COUNT = 'image_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's checkout different columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "brand          0\n",
       "city           0\n",
       "title          0\n",
       "desc           0\n",
       "image_count    0\n",
       "created_at     0\n",
       "price          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                59189\n",
       "unique                   9\n",
       "top       Samsung::سامسونگ\n",
       "freq                 19760\n",
       "Name: brand, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[COLUMN_BRAND].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      59189\n",
       "unique         9\n",
       "top       Tehran\n",
       "freq       21860\n",
       "Name: city, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[COLUMN_CITY].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    59189.000000\n",
       "mean         1.642974\n",
       "std          1.371340\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          2.000000\n",
       "75%          3.000000\n",
       "max         11.000000\n",
       "Name: image_count, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[COLUMN_IMAGE_COUNT].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.918900e+04\n",
       "mean     6.202780e+05\n",
       "std      5.616647e+05\n",
       "min     -1.000000e+00\n",
       "25%      2.000000e+05\n",
       "50%      4.500000e+05\n",
       "75%      9.000000e+05\n",
       "max      2.800000e+06\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[COLUMN_TARGET].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that every cell in our data is filled, we have ads from 9 different cities and 9 different brands. Also we know that products with price of -1, do not have any price specified, so we filter them out, there also some ads with very low price like 10 thousand tomans, which can be advertiser mistake so we will fill them with brand mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5.330100e+04\n",
       "mean     7.037792e+05\n",
       "std      5.422700e+05\n",
       "min      5.010000e+04\n",
       "25%      3.000000e+05\n",
       "50%      5.500000e+05\n",
       "75%      9.700000e+05\n",
       "max      2.800000e+06\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INVALID_PRICE_THRESHOLD = 50000\n",
    "df = df[df[COLUMN_TARGET] > 0]\n",
    "\n",
    "mean_price_by_brand = df.groupby([COLUMN_BRAND]).mean()[COLUMN_TARGET]\n",
    "df[COLUMN_TARGET] = df.apply(\n",
    "    lambda row:\n",
    "        row[COLUMN_TARGET] if row[COLUMN_TARGET] > INVALID_PRICE_THRESHOLD\n",
    "        else mean_price_by_brand[row[COLUMN_BRAND]],\n",
    "    axis=1,\n",
    ")\n",
    "target = df[COLUMN_TARGET]\n",
    "\n",
    "df[COLUMN_TARGET].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have one numerical column (image_count), 2 categorical columns (brand and city), and 2 textual columns (title and description). There is also a column of created_at which in my opinion will not give any information about the price. So we will build out features dataframe from other columns.\n",
    "\n",
    "As we know the models can only process numbers so we have to convert our data to numbers.\n",
    "- For the numerical column `MinMaxScaler` from sklearn will be used.\n",
    "- For the categorical columns we can use `LabelEncoder` and `OneHotEncoder`. `OneHotEncoder` is used here.\n",
    "- For textual columns First the texts must be processed, there is this python library, [`hazm`](https://github.com/sobhe/hazm), for digesting Persian text. The normalizer, lemmatizer, and stopwords list from hazm are used here. The lemmatizer needs postagger trained model from hazm which can be downloaded from [here](https://github.com/sobhe/hazm/releases/download/v0.5/resources-0.5.zip) and must be beside the codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = hazm.Normalizer(persian_numbers=False)\n",
    "post_tagger = hazm.POSTagger(model='./resources-0.5/postagger.model')\n",
    "lemmatizer = hazm.InformalLemmatizer()\n",
    "stopwords = hazm.stopwords_list()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ').replace('/', ' ').replace('آ', 'ا').lower()\n",
    "    result = []\n",
    "    for sentence in hazm.sent_tokenize(normalizer.normalize(text)):\n",
    "        tagged_sentence = [\n",
    "            (word, tag)\n",
    "            for (word, tag) in post_tagger.tag(hazm.word_tokenize(sentence))\n",
    "            if len(word) > 1 and word not in stopwords\n",
    "        ]\n",
    "\n",
    "        for word, tag in tagged_sentence:\n",
    "            if tag:\n",
    "                word = lemmatizer.lemmatize(word, tag)\n",
    "            else:\n",
    "                word = lemmatizer.lemmatize(word)\n",
    "\n",
    "            result.append(unidecode(word))\n",
    "\n",
    "    return result\n",
    "\n",
    "def vectorize_text_column(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(clean_text)\n",
    "    word_counter = Counter()\n",
    "    for _, words in df[column_name].items():\n",
    "        word_counter.update(words)\n",
    "\n",
    "    new_columns = []\n",
    "    most_common_words = [word for word, _ in word_counter.most_common(200)]\n",
    "    for word in most_common_words:\n",
    "        new_column_name = f'has_{word}_in_{column_name}'\n",
    "        new_columns.append(new_column_name)\n",
    "        df[new_column_name] = df[column_name].apply(\n",
    "            lambda x: 1 if x.count(word) > 0 else 0)\n",
    "\n",
    "    return new_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `clean_text` helper function will get a text as its only argument and returns a list of lemmatized words which are not in stopwords list. It uses `hazm` part of speech tagger for better lemmatization. At last the the text are unidecoded to become ascii characters.\n",
    "\n",
    "Then we have other helper function to convert a texttual columnt to a vector of numbers. It first use the `clean_text` to get list of cleaned words of text and then by using python built-in Counter will find 200 most common words from the column. Then at most 200 columns are added to dataframe, which has a value of 1 if that words exists in that textual column and 0 otherwise. Newly added columns are returned at last.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_columns = vectorize_text_column(df, COLUMN_TITLE)\n",
    "description_columns = vectorize_text_column(df, COLUMN_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, city and brand columns are going to be encoded so we will get them to use as column titles, The persian part is also omitted from the brand name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[COLUMN_BRAND] = df[COLUMN_BRAND].apply(lambda brand: brand.split('::')[0])\n",
    "\n",
    "BRANDS = sorted(df[COLUMN_BRAND].unique())\n",
    "CITIES = sorted(df[COLUMN_CITY].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our features dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53301 entries, 0 to 53300\n",
      "Columns: 419 entries, image_count to has_wsh_in_desc\n",
      "dtypes: float64(419)\n",
      "memory usage: 170.4 MB\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.DataFrame(\n",
    "    data=ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                'cleaner',\n",
    "                'drop',\n",
    "                [\n",
    "                    'Unnamed: 0',\n",
    "                    'created_at',\n",
    "                    COLUMN_IMAGE_COUNT,\n",
    "                    COLUMN_TITLE,\n",
    "                    COLUMN_DESCRIPTION,\n",
    "                    COLUMN_TARGET,\n",
    "                ]\n",
    "            ),\n",
    "            (\n",
    "                'scaler',\n",
    "                MinMaxScaler(),\n",
    "                [\n",
    "                    COLUMN_IMAGE_COUNT,\n",
    "                ]\n",
    "            ),\n",
    "            (\n",
    "                'encoder',\n",
    "                OneHotEncoder(),\n",
    "                [\n",
    "                    COLUMN_BRAND,\n",
    "                    COLUMN_CITY,\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        sparse_threshold=0,\n",
    "    ).fit_transform(df),\n",
    "    columns=[\n",
    "        COLUMN_IMAGE_COUNT,\n",
    "        *BRANDS,\n",
    "        *CITIES,\n",
    "        *title_columns,\n",
    "        *description_columns,\n",
    "    ],\n",
    ").infer_objects()\n",
    "features_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model, Training, Evaluation, Hyperparameter tuning\n",
    "Now we have out features and we can choose a model to train. There are different regressors available to use, I've tested 3 different regressors, `LinearRegression` and `Ridge` from `sklearn` and `LGBMRegressor` from `lightgbm`. `Ridge` is just `LinearRegression` with L2 regularization and `LightGBMRegressor` is a gradient boosting model that uses tree-based learning algorithms.\n",
    "\n",
    "Some helper functions are defined and then each model is trained and evaluated to find the best model. `train_test_split` from `sklearn` is used to split the dataset into train and test sets.\n",
    "\n",
    "MAE, mean absolute error and RMSE, root mean squared error are used as metrics to evaluate the models, the lower these metrics are, the better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_df, target)\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'mae': metrics.mean_absolute_error(y_true, y_pred),\n",
    "        'rmse': sqrt(metrics.mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(Model, **kwargs):\n",
    "    model = Model(**kwargs).fit(x_train, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    return Model.__name__, {\n",
    "        'train': calculate_metrics(y_train, y_train_pred),\n",
    "        'test': calculate_metrics(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_hyper_param_index(res):\n",
    "    _, best_param_index = min(\n",
    "        [(v, i) for i, v in enumerate(list(map(lambda x: x[1]['test']['rmse'], res)))]\n",
    "    )\n",
    "    return best_param_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LinearRegression',\n",
       " {'train': {'mae': 211132.58100846468, 'rmse': 297561.85882413964},\n",
       "  'test': {'mae': 210419.85624346536, 'rmse': 296653.4701782954}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ridge',\n",
       " {'train': {'mae': 211168.32239630638, 'rmse': 297564.5153490412},\n",
       "  'test': {'mae': 210441.69538242024, 'rmse': 296660.1494826713}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_res = [\n",
    "    evaluate_model(Ridge, solver=\"sag\", random_state=42, alpha=alpha)\n",
    "    for alpha in [1, 2, 3, 3.5, 4, 4.5, 5, 6, 7]\n",
    "]\n",
    "ridge_res[find_best_hyper_param_index(ridge_res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LGBMRegressor',\n",
       " {'train': {'mae': 186538.6156227561, 'rmse': 269330.4322410771},\n",
       "  'test': {'mae': 193156.03616018448, 'rmse': 279520.24174821447}})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(LGBMRegressor, subsample=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also this method to find the best hyperparameters for LGBM regressor from [this article](https://towardsdatascience.com/mercari-price-suggestion-97ff15840dbd):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  30 | elapsed:  1.7min remaining:   58.9s\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  30 | elapsed:  1.8min remaining:   33.2s\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  30 | elapsed:  2.0min remaining:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.44583275285359114, 'learning_rate': 0.09997491581800289, 'max_depth': 12, 'min_child_weight': 1.7323522915498704, 'n_estimators': 1323, 'num_leaves': 123}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LGBMRegressor',\n",
       " {'train': {'mae': 110422.51712501375, 'rmse': 170358.44297517143},\n",
       "  'test': {'mae': 189351.13891946105, 'rmse': 275360.2751925946}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    'learning_rate': uniform(0, 1),\n",
    "    'n_estimators': sp_randint(200, 1500),\n",
    "    'num_leaves': sp_randint(20, 200),\n",
    "    'max_depth': sp_randint(2, 15),\n",
    "    'min_child_weight': uniform(0, 2),\n",
    "    'colsample_bytree': uniform(0, 1),\n",
    "}\n",
    "best_params = RandomizedSearchCV(\n",
    "    estimator=LGBMRegressor(subsample=0.9), param_distributions=params, n_iter=10, cv=3, random_state=42,\n",
    "    scoring='neg_root_mean_squared_error', verbose=10, return_train_score=True, n_jobs=-1\n",
    ").fit(x_train, y_train).best_params_\n",
    "\n",
    "print(best_params)\n",
    "evaluate_model(LGBMRegressor, **best_params,\n",
    "                     subsample=0.9, random_state=42, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "We can see that the best model is LGBM regressor with tuned hyperparameters. So this model is used as our final model to predict the prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 431100.9737501 , 1288947.54960207,  499408.84773243, ...,\n",
       "       1426617.59646782,  830701.96048179, 1929347.09398051])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor(**best_params, subsample=0.9, random_state=42, n_jobs=-1)\n",
    "model.fit(x_train, y_train)\n",
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some other methods such as TF-IDF to be used as text encoders. We can also extract more features like memory from the text, or if the cellphone is new or has any problems from the text to have better predictions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('venv': venv)",
   "language": "python",
   "name": "python38264bitvenvvenv9709a54fda0c48e7a2344ef7dda77d8c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
